{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f1dace1",
   "metadata": {},
   "source": [
    "# Check and move corrupted Tif files\n",
    "\n",
    "This code scans a directory for TIFF files and detects corrupted ones by verifying and loading each image. It uses a PyTorch DataLoader to process files in batches, handling various exceptions to identify corrupted files accurately. Additionally, it prints the current subdirectory being processed for monitoring purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a10466da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import shutil\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a541428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 32\n"
     ]
    }
   ],
   "source": [
    "num_cores = os.cpu_count()\n",
    "print(f\"Number of CPU cores: {num_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7af981e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the main directory\n",
    "\n",
    "#main_dir = \"../Project003a_Plankton_imager/data_tar/2023-06-07_error\"\n",
    "main_dir = \"data/DETAILED_merged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cc58f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define funtion\n",
    "\n",
    "class TiffDataset(Dataset):\n",
    "    def __init__(self, file_paths):\n",
    "        self.file_paths = file_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        try:\n",
    "            # First, try to open and verify the image\n",
    "            with Image.open(file_path) as img:\n",
    "                img.verify()  # Verify the image integrity\n",
    "            \n",
    "            # Re-open the image and try to load it\n",
    "            with Image.open(file_path) as img:\n",
    "                img.load()  # Load the image to catch truncation issues\n",
    "            return file_path, False  # File is not corrupted\n",
    "        except (IOError, SyntaxError) as e:\n",
    "            # Catch general corruption or syntax issues\n",
    "            print(f\"Corrupted file detected: {file_path}\\nError: {e}\")\n",
    "            return file_path, True  # File is corrupted\n",
    "        except OSError as e:\n",
    "            # Specific handling for truncated files or load issues\n",
    "            print(f\"File loading issue: {file_path}\\nError: {e}\")\n",
    "            return file_path, True  # File is corrupted\n",
    "        except Exception as e:\n",
    "            # Handle any unexpected errors separately\n",
    "            print(f\"Unexpected error for file: {file_path}\\nError: {e}\")\n",
    "            return file_path, True  # Consider unexpected errors as corruption\n",
    "\n",
    "def find_corrupted_tiff_files(main_dir, batch_size=600, num_workers=num_cores):\n",
    "    all_tiff_files = []\n",
    "\n",
    "    # Traverse the directory recursively and gather all TIFF files\n",
    "    for root, _, files in os.walk(main_dir):\n",
    "        print(f\"Processing directory: {root}\")  # Monitor current subdirectory\n",
    "        tiff_files = [os.path.join(root, file) for file in files if file.lower().endswith((\".tiff\", \".tif\"))]\n",
    "        all_tiff_files.extend(tiff_files)\n",
    "\n",
    "    if not all_tiff_files:\n",
    "        print(\"No TIFF files found.\")\n",
    "        return []\n",
    "\n",
    "    # Create a PyTorch Dataset and DataLoader\n",
    "    dataset = TiffDataset(all_tiff_files)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "    corrupted_files = []\n",
    "\n",
    "    # Process the files using the DataLoader\n",
    "    for batch in tqdm(dataloader, desc=\"Processing TIFF files\", unit=\"batch\"):\n",
    "        for file_path, is_corrupted in zip(batch[0], batch[1]):\n",
    "            if is_corrupted:\n",
    "                corrupted_files.append(file_path)\n",
    "                print(f\"\\nCorrupted file found: {file_path}\")\n",
    "\n",
    "    if not corrupted_files:\n",
    "        print(\"No corrupted files found.\")\n",
    "    return corrupted_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da82dc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/WUR/hoeke007/miniconda3/envs/Jupy_Env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: data/DETAILED_merged\n",
      "Processing directory: data/DETAILED_merged/Cnidaria_Hydrozoa-polyp\n",
      "Processing directory: data/DETAILED_merged/Crustacea_Cirripedia-larvae\n",
      "Processing directory: data/DETAILED_merged/artefacts\n",
      "Processing directory: data/DETAILED_merged/Crustacea_Copepoda-nauplii\n",
      "Processing directory: data/DETAILED_merged/Crustacea_Copepoda_Monstrilloidae\n",
      "Processing directory: data/DETAILED_merged/Mollusca_Gastropoda\n",
      "Processing directory: data/DETAILED_merged/Cnidaria_Scyphozoa-ephyrae\n",
      "Processing directory: data/DETAILED_merged/Fish-eggs\n",
      "Processing directory: data/DETAILED_merged/Crustacea_Cladocera\n",
      "Processing directory: data/DETAILED_merged/Crustacea_Copepoda_Calanoida\n",
      "Processing directory: data/DETAILED_merged/Cnidaria_Hydrozoa-medusa\n",
      "Processing directory: data/DETAILED_merged/Bryozoa-larvae\n",
      "Processing directory: data/DETAILED_merged/Platyhelminthes\n",
      "Processing directory: data/DETAILED_merged/Ciliophora\n",
      "Processing directory: data/DETAILED_merged/Echinodermata_Asteroida-larvae\n",
      "Processing directory: data/DETAILED_merged/Branchiostomatidae\n",
      "Processing directory: data/DETAILED_merged/Chaetognatha\n",
      "Processing directory: data/DETAILED_merged/Crustacea_Cumacea\n",
      "Processing directory: data/DETAILED_merged/Crustacea_Copepoda_Cyclopoida\n",
      "Processing directory: data/DETAILED_merged/Cnidaria_Hydrozoa_Siphonophora\n",
      "Processing directory: data/DETAILED_merged/Crustacea_Amphipoda\n",
      "Processing directory: data/DETAILED_merged/Echinodermata_Echinoidea-larvae\n",
      "Processing directory: data/DETAILED_merged/Larvacea\n",
      "Processing directory: data/DETAILED_merged/Invertebrata-eggs\n",
      "Processing directory: data/DETAILED_merged/Crustacea_Copepoda_Harpacticoida\n",
      "Processing directory: data/DETAILED_merged/Brachiopoda-larvae\n",
      "Processing directory: data/DETAILED_merged/Annelida_Polychaeta_Tomopteris\n",
      "Processing directory: data/DETAILED_merged/Echinodermata_Ophiuroidea-larvae\n",
      "Processing directory: data/DETAILED_merged/Crustacea_Euphausidae\n",
      "Processing directory: data/DETAILED_merged/Mollusca_Bivalvia-larvae\n",
      "Processing directory: data/DETAILED_merged/Myzozoa_Dinophyceae\n",
      "Processing directory: data/DETAILED_merged/Ctenophora\n",
      "Processing directory: data/DETAILED_merged/Nemetrea-larvae\n",
      "Processing directory: data/DETAILED_merged/Crustacea_Mysidacea\n",
      "Processing directory: data/DETAILED_merged/Mollusca_Gastropoda-larvae\n",
      "Processing directory: data/DETAILED_merged/Haptophyta_Phaeocystales\n",
      "Processing directory: data/DETAILED_merged/exclude\n",
      "Processing directory: data/DETAILED_merged/Radiozoa\n",
      "Processing directory: data/DETAILED_merged/Crustacea-exuvia\n",
      "Processing directory: data/DETAILED_merged/Fish-larvae\n",
      "Processing directory: data/DETAILED_merged/Annelida_Polychaeta-larvae\n",
      "Processing directory: data/DETAILED_merged/Heterokontophyta_Bacillariophyceae\n",
      "Processing directory: data/DETAILED_merged/Hemichordata_Enteropneusta-larvae\n",
      "Processing directory: data/DETAILED_merged/bubbles\n",
      "Processing directory: data/DETAILED_merged/Detritus\n",
      "Processing directory: data/DETAILED_merged/Tunicata_Doliolida\n",
      "Processing directory: data/DETAILED_merged/Crustacea_Decapoda-larvae\n",
      "Processing directory: data/DETAILED_merged/Tunicata_Ascidiacea-larvae\n",
      "Processing directory: data/DETAILED_merged/Cyanobacteria\n",
      "Processing directory: data/DETAILED_merged/Phoronida-larvae\n",
      "Processing directory: data/DETAILED_merged/Gastropoda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing TIFF files: 100%|██████████| 95/95 [00:08<00:00, 10.74batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No corrupted files found.\n",
      "CPU times: user 267 ms, sys: 154 ms, total: 421 ms\n",
      "Wall time: 9.02 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Run the function\n",
    "corrupted_files = find_corrupted_tiff_files(main_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0af36ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrupted files have been exported to corrupted_files_v01.csv.\n"
     ]
    }
   ],
   "source": [
    "# Export filenames of corrupted files to csv file\n",
    "\n",
    "# Specify the output CSV file path\n",
    "output_csv_path = \"corrupted_files_v01.csv\"\n",
    "\n",
    "# Write the corrupted files to the CSV file\n",
    "with open(output_csv_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Corrupted File Path\"])  # Write header\n",
    "    for file_path in corrupted_files:\n",
    "        writer.writerow([file_path])\n",
    "\n",
    "print(f\"Corrupted files have been exported to {output_csv_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b38469c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File moving process is complete.\n"
     ]
    }
   ],
   "source": [
    "# Move corrupted files to new folder\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = \"corrupted_files_v01.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Specify the new folder where you want to move the corrupted files\n",
    "new_folder_path = \"corrupted_files_v01\"\n",
    "os.makedirs(new_folder_path, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "# Iterate over the file paths and move each file to the new folder\n",
    "for file_path in df[\"Corrupted File Path\"]:\n",
    "    try:\n",
    "        # Move the file\n",
    "        shutil.move(file_path, new_folder_path)\n",
    "        print(f\"Moved: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error moving {file_path}: {e}\")\n",
    "\n",
    "print(\"File moving process is complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4551d239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate files found.\n"
     ]
    }
   ],
   "source": [
    "# Checking for duplicate files\n",
    "\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "def hash_file(filepath, block_size=65536):\n",
    "    hasher = hashlib.sha256()\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        for block in iter(lambda: f.read(block_size), b\"\"):\n",
    "            hasher.update(block)\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "# Folder to check\n",
    "base_folder = Path(\"data\") / \"WMR\"\n",
    "\n",
    "# Dictionary to group files by their hash\n",
    "hash_to_files = defaultdict(list)\n",
    "\n",
    "# Walk through all files recursively\n",
    "for file in base_folder.rglob(\"*\"):\n",
    "    if file.is_file():\n",
    "        file_hash = hash_file(file)\n",
    "        hash_to_files[file_hash].append(file)\n",
    "\n",
    "# Find and print duplicates\n",
    "duplicates_found = False\n",
    "for file_list in hash_to_files.values():\n",
    "    if len(file_list) > 1:\n",
    "        duplicates_found = True\n",
    "        print(\"Duplicate files:\")\n",
    "        for path in file_list:\n",
    "            print(f\"  - {path}\")\n",
    "        print()\n",
    "\n",
    "if not duplicates_found:\n",
    "    print(\"No duplicate files found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169cee2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a387301f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupy_Env",
   "language": "python",
   "name": "jupy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
